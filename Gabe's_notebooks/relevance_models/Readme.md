This folder contains all of my attempts to find a model which can find relevant posts in the r/bpd subreddit. All of these (except for the ollama model) were training on the manually coded sample from the Kaggle dataset. However, they can be run on the full dataset. Here is a summary of each model and their performance.

1. The baseline model processes the title and text by removing stopwords, punctuation, etc. Then it uses Tdidf vectorization on the cleaned text and runs logistic regression on the 'highly_relevant' variable. Since only 21 of 600 posts were highly relevant, it was necessary to stratify the test-train split and set a very low threshold for relevance. I set the threshold to .04, and doing so I got approximately 75% recall and 10% precision. However, since the variable we were regressing on was manually coded to be narrowly construed, many of these 'false positives' still may contain useful information to the question at hand. Furthermore, this analysis allowed us to see the words that were commonly being used in the highly relevant posts.

2. Using the list of keywords provided by Eunbin and filling them out with a large number of synonyms, some exploratory data analysis revealed that posts which did not use one of these keywords were highly unlikely to be relevant. Furthermore, posts which mention a medication by name were very likely to be relevant. Mentioning therapy techniques by name was also correlated with relevance, but less strongly so. This insight lead to the next class of models, which counted the number of keywords in a variety of classes (i.e. medications, therapy models, etc.) to try to determine relevance. With the same seed, this model had 88% recall and 39% precision.

3. However, this approach still did not fully account for the 'zero-inflation' aspect of the data, where the absence of a keyword was a very good indicator that the post was not relevant. Therefore, I created a set of models called 'decision tree' models because the first step that they did was to search the cleaned text for any keyword. If none was found, then they would return 0 for relevance. For posts containing the keyword, they would run some type of regression on the keyword count and post text. I experimented with a number of possibilities here, including the following.
    a. Using different types of regression for the non-rejected posts. In most cases, logistic regression performed the best. The average performance was slighly better than the models in step 2, but for some seeds which were difficult for all the models, there was a sizable difference.
    b. Including a count of negative keywords which were negatively associated with relevance (and whose presence was not considered in the first step). These negative keywords were determined by the baseline model. In some cases this improved the model, but not consistently enough to warrant the additional complexity.
    c. Not counting the general keywords, but just using their presence to not immediately reject a post. This did not improve upon the keyword model where general keywords were counted.

4. To compare this model with a more sophisticated model, I used the coded dataset to train a fine-tuning of distilBert. I originally tried BERT, but it was too computationally intractable. However, this model is performing worse than the keyword model. I believe the reason is that we are looking for something very specific (posts which discuss their history of treatment with bpd) and the training set was simply too small for this purpose. Furthermore, this model was too large to upload to Github, so if we want to have access to it, I'll find some other way to post it.

5. Finally, I used a large language model to analyze the content of each post and code the posts as relevant or not. While the results of this analysis were extremely accurate in each of the cases that were tested, the analysis took 2 to 3 minutes per post, so this was not feasible for the larger dataset. Therefore, the final relevance model was the decision tree with three categories of keywords; medications, treatments (indicating therapy techniques), and general keywords (e.g., "treatment", "therapist") which were often used in relevant posts though not as specific as the others. For most random seeds, the performance of this model was similar to the keyword model (though slightly better). The major improvement was that there were random seeds, nearly all the models would struggle considerably but this model's performance was not as poor as the others.