{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree model\n",
    "\n",
    "This model uses a keyword search to determine if posts contain a keyword. If not, it marks them as irrelevant. If they do contain a keyword, it performs logistic regression to determine relevance. To improve performance, it also takes various lists of keywords (such as medications, therapies, etc) and a list of negative keywords which are negatively associated with relevance (e.g. 'dating', 'fp'). These negative keywords were determined by the baseline model, but can easily be altered to improve (or worsen) model performance. In addition, there is a parameter called threshold which determines the probability needed to mark a post as relevant or not. Right now, it is set to .1 (although it might be better to make it even smaller). Feel free to alter the keywords.\n",
    "\n",
    "Right now, the model incorporates the test train split within the model, and at some point I'll try to remove that without breaking everything, but my initial attempts to do so did not work. In fact, there are a number of features which are somewhat awkwardly built into the class, which makes this somewhat impractical to use. However, we should be able to improve it with a bit more care in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "class TextRelevanceModel:\n",
    "    def __init__(self, keyword_categories, negative_keywords=None, model=None):\n",
    "        \"\"\"\n",
    "        Initialize the model with keyword categories and an optional model.\n",
    "        \n",
    "        :param keyword_categories: Dictionary where keys are category names and values are lists of keywords.\n",
    "        :param negative_keywords: List of negative keywords.\n",
    "        :param model: An optional machine learning model. Defaults to LogisticRegression.\n",
    "        \"\"\"\n",
    "        self.keyword_categories = keyword_categories\n",
    "        self.negative_keywords = negative_keywords if negative_keywords is not None else []\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.model = model if model is not None else LogisticRegression(max_iter=1000, random_state=42)\n",
    "    \n",
    "    def keyword_count(self, text, keywords):\n",
    "        \"\"\"Count the number of keywords present in the text.\"\"\"\n",
    "        return sum(1 for word in text.lower().split() if word in keywords)\n",
    "    \n",
    "    def prepare_data(self, df, text_column, target_column):\n",
    "        \"\"\"Prepare features and target variable from the dataframe.\"\"\"\n",
    "        # Initialize a DataFrame to store keyword counts for each category\n",
    "        keyword_counts = pd.DataFrame()\n",
    "        \n",
    "        for category, keywords in self.keyword_categories.items():\n",
    "            keyword_counts[category + '_count'] = df[text_column].apply(lambda x: self.keyword_count(x, keywords))\n",
    "        \n",
    "        # Calculate negative keyword counts\n",
    "        keyword_counts['negative_keyword_count'] = df[text_column].apply(lambda x: self.keyword_count(x, self.negative_keywords))\n",
    "        \n",
    "        # Vectorize the text data\n",
    "        X_text = self.vectorizer.fit_transform(df[text_column])\n",
    "        \n",
    "        # Combine the text features and the keyword count features\n",
    "        X_keywords = keyword_counts.to_numpy()\n",
    "        X = hstack([X_text, csr_matrix(X_keywords)])\n",
    "        \n",
    "        # Target variable\n",
    "        y = df[target_column]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Train the model using the provided features and target variable.\"\"\"\n",
    "        # Filter out rows where no keywords are present in any category\n",
    "        keyword_present = np.any(X[:, -len(self.keyword_categories):].toarray(), axis=1)\n",
    "        X_keyword = X[keyword_present]\n",
    "        y_keyword = y[keyword_present]\n",
    "        \n",
    "        # Fit the model on the filtered data\n",
    "        self.model.fit(X_keyword, y_keyword)\n",
    "    \n",
    "    def predict_proba(self, text):\n",
    "        \"\"\"Predict the probability of relevance for a new text.\"\"\"\n",
    "        # Vectorize the input text\n",
    "        X_text_new = self.vectorizer.transform([text])\n",
    "        \n",
    "        # Calculate keyword counts for the new text\n",
    "        keyword_counts_new = np.array([[self.keyword_count(text, keywords) for keywords in self.keyword_categories.values()]])\n",
    "        negative_keyword_count_new = np.array([[self.keyword_count(text, self.negative_keywords)]])\n",
    "        \n",
    "        # Combine features\n",
    "        X_new = hstack([X_text_new, csr_matrix(keyword_counts_new), csr_matrix(negative_keyword_count_new)])\n",
    "        \n",
    "        # Check if any keyword from the categories is present\n",
    "        if not np.any(keyword_counts_new):\n",
    "            return 0.0\n",
    "        \n",
    "        # Predict using the model\n",
    "        return self.model.predict_proba(X_new)[0, 1]  # Return the probability of the positive class\n",
    "    \n",
    "    def evaluate(self, df, text_column, target_column, threshold=0.5):\n",
    "        \"\"\"Evaluate the model performance on the provided dataframe.\"\"\"\n",
    "        X, y = self.prepare_data(df, text_column, target_column)\n",
    "        \n",
    "        # Split the data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, stratify=y, random_state=57)\n",
    "        \n",
    "        # Train the model on the training set\n",
    "        self.train(X_train, y_train)\n",
    "        \n",
    "        # Predict probabilities on the test set\n",
    "        y_proba = np.array([self.predict_proba(text) for text in df[text_column].iloc[y_test.index]])\n",
    "        \n",
    "        # Apply the threshold to get the predicted classes\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        \n",
    "        # Print the classification report\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Print the confusion matrix\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96       290\n",
      "           1       0.29      0.90      0.44        10\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.64      0.91      0.70       300\n",
      "weighted avg       0.97      0.92      0.94       300\n",
      "\n",
      "[[268  22]\n",
      " [  1   9]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gkhan/Library/Python/3.12/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91       290\n",
      "           1       0.12      0.60      0.20        10\n",
      "\n",
      "    accuracy                           0.84       300\n",
      "   macro avg       0.55      0.72      0.56       300\n",
      "weighted avg       0.96      0.84      0.89       300\n",
      "\n",
      "[[246  44]\n",
      " [  4   6]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Importing and dropping rows from ataFrame\n",
    "\n",
    "    df_coded = pd.read_csv('../data/processed_and_coded_posts.csv')\n",
    "    df = df_coded[['processed_text','highly_relevant']]\n",
    "    \n",
    "    #Importing keywords\n",
    "\n",
    "    csv_file_path = '../keywords/medications.csv'\n",
    "\n",
    "    # Read the CSV file\n",
    "    df_med = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Extract the first column as a list of keywords\n",
    "    medications = df_med.iloc[:, 0].tolist()\n",
    "\n",
    "    csv_file_path_2 = '../keywords/Treatment.csv'\n",
    "\n",
    "    # Read the CSV file\n",
    "    df_therapy = pd.read_csv(csv_file_path_2)\n",
    "\n",
    "    # Extract the first column as a list of keywords\n",
    "    therapy = df_therapy.iloc[:, 0].tolist()\n",
    "\n",
    "    general_keywords = ['medicine','therapy','treatment','recovery','prescribed','diagnosed','med','meds','prescribe','therapist','session','psychiatrist','psychiatrists','dosage','medication', 'dbt', 'abilify', 'outpatient', 'therapist', 'harming','medicine','therapy','treatment','recovery','prescribed','diagnosed','therapists','prescribe','diagnose','medicines','drugs','drug','therapist','session']\n",
    "\n",
    "    # Define categories of keywords\n",
    "    \n",
    "    keyword_categories = {\n",
    "    'general_keywords': general_keywords,\n",
    "    'medications': medications,\n",
    "    'therapy': therapy\n",
    "    }\n",
    "    \n",
    "    # Define negative keywords\n",
    "    negative_keywords = ['relationship', 'friend', 'together', 'fp', 'people', 'person', 'partner', 'dating']\n",
    "    \n",
    "    \n",
    "\n",
    " # Create an instance of the model\n",
    "    model = TextRelevanceModel(keyword_categories, negative_keywords)\n",
    "    model2 = TextRelevanceModel(keyword_categories, negative_keywords,AdaBoostClassifier())\n",
    "\n",
    "\n",
    "    # Evaluate the model on the DataFrame with a specified threshold to increase sensitivity\n",
    "    model.evaluate(df, text_column='processed_text', target_column='highly_relevant', threshold=0.05)\n",
    "    model2.evaluate(df, text_column='processed_text', target_column='highly_relevant', threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'key of type tuple not found and not a MultiIndex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhighly_relevant\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 53\u001b[0m, in \u001b[0;36mTextRelevanceModel.train\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train the model using the provided features and target variable.\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Filter out rows where no keywords are present in any category\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m keyword_present \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39many(\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeyword_categories\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     54\u001b[0m X_keyword \u001b[38;5;241m=\u001b[39m X[keyword_present]\n\u001b[1;32m     55\u001b[0m y_keyword \u001b[38;5;241m=\u001b[39m y[keyword_present]\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pandas/core/series.py:1144\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rows_with_mask(key)\n\u001b[0;32m-> 1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pandas/core/series.py:1154\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexing a Series with DataFrame is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported, use the appropriate DataFrame column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1152\u001b[0m     )\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m-> 1154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_values_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key):\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;66;03m# e.g. scalars that aren't recognized by lib.is_scalar, GH#32684\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc[key]\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pandas/core/series.py:1198\u001b[0m, in \u001b[0;36mSeries._get_values_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m-> 1198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey of type tuple not found and not a MultiIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;66;03m# If key is contained, would have returned by now\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m indexer, new_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc_level(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'key of type tuple not found and not a MultiIndex'"
     ]
    }
   ],
   "source": [
    "model.train(df.processed_text,df.highly_relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_text\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 77\u001b[0m, in \u001b[0;36mTextRelevanceModel.predict_proba\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Predict using the model\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/sklearn/linear_model/_logistic.py:1367\u001b[0m, in \u001b[0;36mLogisticRegression.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;124;03m    Probability estimates.\u001b[39;00m\n\u001b[1;32m   1344\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;124;03m        where classes are ordered as they are in ``self.classes_``.\u001b[39;00m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1367\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1369\u001b[0m     ovr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1370\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1371\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         )\n\u001b[1;32m   1375\u001b[0m     )\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ovr:\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/sklearn/utils/validation.py:1622\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[0;32m-> 1622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.predict_proba(df.processed_text[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mprepare_data(df, text_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m, target_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighly_relevant\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Predict probabilities on the entire dataset\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m y_proba \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Apply a threshold to determine relevant entries\u001b[39;00m\n\u001b[1;32m     11\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n",
      "Cell \u001b[0;32mIn[42], line 63\u001b[0m, in \u001b[0;36mTextRelevanceModel.predict_proba\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict the probability of relevance for a new text.\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Vectorize the input text\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m X_text_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Calculate keyword counts for the new text\u001b[39;00m\n\u001b[1;32m     66\u001b[0m keyword_counts_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeyword_count(text, keywords) \u001b[38;5;28;01mfor\u001b[39;00m keywords \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeyword_categories\u001b[38;5;241m.\u001b[39mvalues()]])\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/sklearn/feature_extraction/text.py:2162\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[1;32m   2146\u001b[0m \n\u001b[1;32m   2147\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2158\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   2159\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2160\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2162\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/sklearn/feature_extraction/text.py:1434\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1434\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1436\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/sklearn/feature_extraction/text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/sklearn/feature_extraction/text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Prepare data\n",
    "X = model.prepare_data(df, text_column='processed_text', target_column='highly_relevant')\n",
    "\n",
    "# Predict probabilities on the entire dataset\n",
    "y_proba = model.predict_proba(\"This is a test of what will happen.\")\n",
    "\n",
    "# Apply a threshold to determine relevant entries\n",
    "threshold = 0.1\n",
    "relevant_indices = np.where(y_proba >= threshold)[0]\n",
    "\n",
    "# Filter the DataFrame based on relevant indices\n",
    "relevant_entries = df.iloc[relevant_indices]\n",
    "\n",
    "# Print or inspect the relevant entries\n",
    "print(relevant_entries)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
