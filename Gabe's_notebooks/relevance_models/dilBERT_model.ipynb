{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trying to build a model with dilBERT\n",
    "\n",
    "I tried to get a classification model going using BERT, but my computer isn't powerful enough to handle it. So here's a version using distillBert, which is smaller and hopefully more usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('../data/sample_posts_manual_coding_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/j4gyltgs6txd83bg66_mv4fn664jzt/T/ipykernel_46462/2380068640.py:7: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df.highly_relevant[598] = np.nan\n",
      "/var/folders/9j/j4gyltgs6txd83bg66_mv4fn664jzt/T/ipykernel_46462/2380068640.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.highly_relevant[598] = np.nan\n",
      "/var/folders/9j/j4gyltgs6txd83bg66_mv4fn664jzt/T/ipykernel_46462/2380068640.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['highly_relevant'] = df['highly_relevant'].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "posts_analyzed = 600\n",
    "\n",
    "df = df1.head(posts_analyzed)\n",
    "\n",
    "## Removing the marker of where things were left off.\n",
    "\n",
    "df.highly_relevant[598] = np.nan\n",
    "\n",
    "df['highly_relevant'] = df['highly_relevant'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/j4gyltgs6txd83bg66_mv4fn664jzt/T/ipykernel_46462/4079358029.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df['highly_relevant'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "df['labels'] = df['highly_relevant'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9j/j4gyltgs6txd83bg66_mv4fn664jzt/T/ipykernel_46462/502628292.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['combined_text'] = df['title'] + ' ' + df['selftext']\n",
      "/var/folders/9j/j4gyltgs6txd83bg66_mv4fn664jzt/T/ipykernel_46462/502628292.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['combined_text'].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "df['combined_text'] = df['title'] + ' ' + df['selftext']\n",
    "\n",
    "# Punctuation removal\n",
    "def remove_punctuation(text):\n",
    "    return ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = remove_punctuation(text).split()\n",
    "    \n",
    "    # Lowercase and remove stopwords\n",
    "    tokens = [word.lower() for word in tokens] # if word.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the combined text column\n",
    "df['text'] = df['combined_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>over_18</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>date_created</th>\n",
       "      <th>self</th>\n",
       "      <th>is_relevant</th>\n",
       "      <th>highly_relevant</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>I'm depressed and got intense fear of abandonm...</td>\n",
       "      <td>last few days i'm just sad and extremely anxio...</td>\n",
       "      <td>1643579271</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>2022-01-30 21:47:51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm depressed and got intense fear of abandonm...</td>\n",
       "      <td>im depressed and got intense fear of abandonme...</td>\n",
       "      <td>1</td>\n",
       "      <td>im depressed and got intense fear of abandonme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>My BF Always Criticizes(?) Me When I Let Him K...</td>\n",
       "      <td>so i'm going to do my best to explain this? it...</td>\n",
       "      <td>1643578689</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>2022-01-30 21:38:09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>My BF Always Criticizes(?) Me When I Let Him K...</td>\n",
       "      <td>my bf always criticizes me when i let him know...</td>\n",
       "      <td>0</td>\n",
       "      <td>my bf always criticizes me when i let him know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>Husband unintentionally stopped taking his med...</td>\n",
       "      <td>my husband has bpd, which, until recently, has...</td>\n",
       "      <td>1643577150</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>2022-01-30 21:12:30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Husband unintentionally stopped taking his med...</td>\n",
       "      <td>husband unintentionally stopped taking his med...</td>\n",
       "      <td>0</td>\n",
       "      <td>husband unintentionally stopped taking his med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>Splitting</td>\n",
       "      <td>i don’t even know if i’m borderline, i just kn...</td>\n",
       "      <td>1643575704</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>2022-01-30 20:48:24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Splitting i don’t even know if i’m borderline,...</td>\n",
       "      <td>splitting i don’t even know if i’m borderline ...</td>\n",
       "      <td>0</td>\n",
       "      <td>splitting i don’t even know if i’m borderline ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Had a fight with partner/FP last night... Stil...</td>\n",
       "      <td>so yeah we can an argument last night related ...</td>\n",
       "      <td>1643573845</td>\n",
       "      <td>False</td>\n",
       "      <td>BPD</td>\n",
       "      <td>2022-01-30 20:17:25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Had a fight with partner/FP last night... Stil...</td>\n",
       "      <td>had a fight with partnerfp last night still an...</td>\n",
       "      <td>0</td>\n",
       "      <td>had a fight with partnerfp last night still an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "595  I'm depressed and got intense fear of abandonm...   \n",
       "596  My BF Always Criticizes(?) Me When I Let Him K...   \n",
       "597  Husband unintentionally stopped taking his med...   \n",
       "598                                          Splitting   \n",
       "599  Had a fight with partner/FP last night... Stil...   \n",
       "\n",
       "                                              selftext  created_utc  over_18  \\\n",
       "595  last few days i'm just sad and extremely anxio...   1643579271    False   \n",
       "596  so i'm going to do my best to explain this? it...   1643578689    False   \n",
       "597  my husband has bpd, which, until recently, has...   1643577150    False   \n",
       "598  i don’t even know if i’m borderline, i just kn...   1643575704    False   \n",
       "599  so yeah we can an argument last night related ...   1643573845    False   \n",
       "\n",
       "    subreddit         date_created  self  is_relevant  highly_relevant  \\\n",
       "595       BPD  2022-01-30 21:47:51     1            1                1   \n",
       "596       BPD  2022-01-30 21:38:09     1            0                0   \n",
       "597       BPD  2022-01-30 21:12:30     0            0                0   \n",
       "598       BPD  2022-01-30 20:48:24     1            0                0   \n",
       "599       BPD  2022-01-30 20:17:25     1            0                0   \n",
       "\n",
       "                                         combined_text  \\\n",
       "595  I'm depressed and got intense fear of abandonm...   \n",
       "596  My BF Always Criticizes(?) Me When I Let Him K...   \n",
       "597  Husband unintentionally stopped taking his med...   \n",
       "598  Splitting i don’t even know if i’m borderline,...   \n",
       "599  Had a fight with partner/FP last night... Stil...   \n",
       "\n",
       "                                        processed_text  labels  \\\n",
       "595  im depressed and got intense fear of abandonme...       1   \n",
       "596  my bf always criticizes me when i let him know...       0   \n",
       "597  husband unintentionally stopped taking his med...       0   \n",
       "598  splitting i don’t even know if i’m borderline ...       0   \n",
       "599  had a fight with partnerfp last night still an...       0   \n",
       "\n",
       "                                                  text  \n",
       "595  im depressed and got intense fear of abandonme...  \n",
       "596  my bf always criticizes me when i let him know...  \n",
       "597  husband unintentionally stopped taking his med...  \n",
       "598  splitting i don’t even know if i’m borderline ...  \n",
       "599  had a fight with partnerfp last night still an...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify = df['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 480/480 [00:00<00:00, 907.02 examples/s]\n",
      "Map: 100%|██████████| 120/120 [00:00<00:00, 860.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Convert datasets to tokenized format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "def tokenize_data(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_data, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/gkhan/Library/Python/3.12/lib/python/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                       \n",
      "  0%|          | 0/300 [28:04<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2025, 'grad_norm': 2.57312273979187, 'learning_rate': 0.00016, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                       \n",
      "\u001b[A                                               \n",
      "\n",
      "  0%|          | 0/300 [29:11<?, ?it/s]        \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17083123326301575, 'eval_runtime': 66.982, 'eval_samples_per_second': 1.792, 'eval_steps_per_second': 0.224, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \n",
      "  0%|          | 0/300 [46:00<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1677, 'grad_norm': 2.6016523838043213, 'learning_rate': 0.00012, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                       \n",
      "\u001b[A                                              \n",
      "\n",
      "  0%|          | 0/300 [47:02<?, ?it/s]        \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16156624257564545, 'eval_runtime': 62.0109, 'eval_samples_per_second': 1.935, 'eval_steps_per_second': 0.242, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \n",
      "  0%|          | 0/300 [59:49<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1637, 'grad_norm': 0.15527881681919098, 'learning_rate': 8e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                       \n",
      "\u001b[A                                              \n",
      "\n",
      "  0%|          | 0/300 [1:00:38<?, ?it/s]      \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1697501391172409, 'eval_runtime': 48.9892, 'eval_samples_per_second': 2.45, 'eval_steps_per_second': 0.306, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \n",
      "  0%|          | 0/300 [1:11:10<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1653, 'grad_norm': 0.299267441034317, 'learning_rate': 4e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                         \n",
      "\u001b[A                                                \n",
      "\n",
      "  0%|          | 0/300 [1:11:56<?, ?it/s]      \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1586875170469284, 'eval_runtime': 46.1768, 'eval_samples_per_second': 2.599, 'eval_steps_per_second': 0.325, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \n",
      "  0%|          | 0/300 [1:22:36<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.17, 'grad_norm': 0.25765475630760193, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                         \n",
      "\u001b[A                                                \n",
      "\n",
      "  0%|          | 0/300 [1:23:21<?, ?it/s]      \n",
      "\u001b[A\n",
      "                                         \n",
      "100%|██████████| 300/300 [1:17:19<00:00, 15.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15938633680343628, 'eval_runtime': 44.9434, 'eval_samples_per_second': 2.67, 'eval_steps_per_second': 0.334, 'epoch': 5.0}\n",
      "{'train_runtime': 4639.6637, 'train_samples_per_second': 0.517, 'train_steps_per_second': 0.065, 'train_loss': 0.17384321848551432, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# Load pre-trained DistilBERT model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Prepare data collator for padding sequences\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Define Trainer object for training the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model\n",
    "trainer.save_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:47<00:00,  3.16s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.15938633680343628,\n",
       " 'eval_runtime': 50.1621,\n",
       " 'eval_samples_per_second': 2.392,\n",
       " 'eval_steps_per_second': 0.299,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = AutoModelForSequenceClassification.from_pretrained('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_relevant = \"I have found dbt to be very helpful, as has lamictal. Taking medication is crucial for me.\"\n",
    "text_irrelevant = \"My fp went away and now I am sad. What should I do? Here is a random mention of therapy to throw the model off\"\n",
    "text_random = \"The quick brown fox jumped over the lazy dog\"\n",
    "\n",
    "\n",
    "\n",
    "encoding = tokenizer(text_irrelevant, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1355, -2.2658]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely enough, this model is performing worse than the keyword model. I believe the reason is that we are looking for something very specific (posts which discuss their history of treatment with bpd) and the training set was simply too small for this purpose. Therefore, an approach using keywords did a bit better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
