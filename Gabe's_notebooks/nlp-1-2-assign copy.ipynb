{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4579285,"sourceType":"datasetVersion","datasetId":2671334},{"sourceId":8161593,"sourceType":"datasetVersion","datasetId":4828752}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"` Data cleaning is a time consuming and unenjoyable task, yet it's a very important one. Keep in mind, \"garbage in, garbage out\".`\n\n#### Feeding dirty data into a model will give us results that are meaningless.\n\n### Objective:\n\n1. Getting the data \n2. Cleaning the data \n3. Organizing the data - organize the cleaned data into a way that is easy to input into other algorithms\n\n### Output :\n#### cleaned and organized data in two standard text formats:\n\n\n- Document-Term Matrix - word counts in matrix format","metadata":{}},{"cell_type":"markdown","source":"## Problem Statement","metadata":{}},{"cell_type":"markdown","source":"Sentiment Analysis: Analyzing the sentiment of posts helps us understand the emotional tone expressed in the text. This could be particularly useful in identifying posts that express negative emotions associated with mental health issues.","metadata":{}},{"cell_type":"markdown","source":"[Link to Mental Disorders Identification Reddit NLP dataset](https://www.kaggle.com/datasets/kamaruladha/mental-disorders-identification-reddit-nlp)","metadata":{}},{"cell_type":"markdown","source":"# CODE","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:06.851572Z","iopub.execute_input":"2024-04-26T08:39:06.852534Z","iopub.status.idle":"2024-04-26T08:39:07.599180Z","shell.execute_reply.started":"2024-04-26T08:39:06.852492Z","shell.execute_reply":"2024-04-26T08:39:07.597541Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/mental-disorders-identification-reddit-nlp/mental_disorders_reddit.csv\n/kaggle/input/mental-disorders-reddit/mental_disorders_reddit.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:07.602108Z","iopub.execute_input":"2024-04-26T08:39:07.603308Z","iopub.status.idle":"2024-04-26T08:39:07.610946Z","shell.execute_reply.started":"2024-04-26T08:39:07.603257Z","shell.execute_reply":"2024-04-26T08:39:07.608859Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df1=pd.read_csv('/kaggle/input/mental-disorders-reddit/mental_disorders_reddit.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:07.612857Z","iopub.execute_input":"2024-04-26T08:39:07.613363Z","iopub.status.idle":"2024-04-26T08:39:27.372568Z","shell.execute_reply.started":"2024-04-26T08:39:07.613312Z","shell.execute_reply":"2024-04-26T08:39:27.370656Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df1.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:27.375864Z","iopub.execute_input":"2024-04-26T08:39:27.376335Z","iopub.status.idle":"2024-04-26T08:39:27.406695Z","shell.execute_reply.started":"2024-04-26T08:39:27.376301Z","shell.execute_reply":"2024-04-26T08:39:27.405458Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                 title  \\\n0  Life is so pointless without others   \n1                           Cold rage?   \n2                I don‚Äôt know who I am   \n3              HELP! Opinions! Advice!   \n4                                 help   \n\n                                            selftext  created_utc  over_18  \\\n0  Does anyone else think the most important part...   1650356960    False   \n1  Hello fellow friends üòÑ\\n\\nI'm on the BPD spect...   1650356660    False   \n2  My [F20] bf [M20] told me today (after I said ...   1650355379    False   \n3  Okay, I‚Äôm about to open up about many things I...   1650353430    False   \n4                                          [removed]   1650350907    False   \n\n  subreddit  \n0       BPD  \n1       BPD  \n2       BPD  \n3       BPD  \n4       BPD  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>selftext</th>\n      <th>created_utc</th>\n      <th>over_18</th>\n      <th>subreddit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Life is so pointless without others</td>\n      <td>Does anyone else think the most important part...</td>\n      <td>1650356960</td>\n      <td>False</td>\n      <td>BPD</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cold rage?</td>\n      <td>Hello fellow friends üòÑ\\n\\nI'm on the BPD spect...</td>\n      <td>1650356660</td>\n      <td>False</td>\n      <td>BPD</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I don‚Äôt know who I am</td>\n      <td>My [F20] bf [M20] told me today (after I said ...</td>\n      <td>1650355379</td>\n      <td>False</td>\n      <td>BPD</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HELP! Opinions! Advice!</td>\n      <td>Okay, I‚Äôm about to open up about many things I...</td>\n      <td>1650353430</td>\n      <td>False</td>\n      <td>BPD</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>help</td>\n      <td>[removed]</td>\n      <td>1650350907</td>\n      <td>False</td>\n      <td>BPD</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(df1.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:27.408634Z","iopub.execute_input":"2024-04-26T08:39:27.409377Z","iopub.status.idle":"2024-04-26T08:39:27.416488Z","shell.execute_reply.started":"2024-04-26T08:39:27.409342Z","shell.execute_reply":"2024-04-26T08:39:27.414900Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(701787, 5)\n","output_type":"stream"}]},{"cell_type":"code","source":"df1.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:27.418983Z","iopub.execute_input":"2024-04-26T08:39:27.420328Z","iopub.status.idle":"2024-04-26T08:39:27.773778Z","shell.execute_reply.started":"2024-04-26T08:39:27.420274Z","shell.execute_reply":"2024-04-26T08:39:27.772224Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 701787 entries, 0 to 701786\nData columns (total 5 columns):\n #   Column       Non-Null Count   Dtype \n---  ------       --------------   ----- \n 0   title        701741 non-null  object\n 1   selftext     668096 non-null  object\n 2   created_utc  701787 non-null  int64 \n 3   over_18      701787 non-null  bool  \n 4   subreddit    701787 non-null  object\ndtypes: bool(1), int64(1), object(3)\nmemory usage: 22.1+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"df1.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:27.776405Z","iopub.execute_input":"2024-04-26T08:39:27.776957Z","iopub.status.idle":"2024-04-26T08:39:27.787989Z","shell.execute_reply.started":"2024-04-26T08:39:27.776912Z","shell.execute_reply":"2024-04-26T08:39:27.786444Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"title          object\nselftext       object\ncreated_utc     int64\nover_18          bool\nsubreddit      object\ndtype: object"},"metadata":{}}]},{"cell_type":"code","source":"df1.isnull().any()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:27.790165Z","iopub.execute_input":"2024-04-26T08:39:27.790709Z","iopub.status.idle":"2024-04-26T08:39:28.117899Z","shell.execute_reply.started":"2024-04-26T08:39:27.790663Z","shell.execute_reply":"2024-04-26T08:39:28.116050Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"title           True\nselftext        True\ncreated_utc    False\nover_18        False\nsubreddit      False\ndtype: bool"},"metadata":{}}]},{"cell_type":"code","source":"df1.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:28.119380Z","iopub.execute_input":"2024-04-26T08:39:28.119928Z","iopub.status.idle":"2024-04-26T08:39:28.445571Z","shell.execute_reply.started":"2024-04-26T08:39:28.119880Z","shell.execute_reply":"2024-04-26T08:39:28.443625Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"title             46\nselftext       33691\ncreated_utc        0\nover_18            0\nsubreddit          0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"df1=df1.dropna(how='any')","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:28.452590Z","iopub.execute_input":"2024-04-26T08:39:28.453031Z","iopub.status.idle":"2024-04-26T08:39:28.868472Z","shell.execute_reply.started":"2024-04-26T08:39:28.452998Z","shell.execute_reply":"2024-04-26T08:39:28.866632Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df1['subreddit'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:28.870055Z","iopub.execute_input":"2024-04-26T08:39:28.870475Z","iopub.status.idle":"2024-04-26T08:39:28.993545Z","shell.execute_reply.started":"2024-04-26T08:39:28.870442Z","shell.execute_reply":"2024-04-26T08:39:28.991843Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"subreddit\nBPD              233119\nAnxiety          167032\ndepression       156708\nbipolar           46666\nmentalillness     44249\nschizophrenia     20280\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Dropping the data points with null values \ndf1 = df1.dropna(how = 'any', axis = 0)\n# lowercasing the column names so it will be easier for access ^^\ndf1.columns = df1.columns.str.lower()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:28.996396Z","iopub.execute_input":"2024-04-26T08:39:28.997556Z","iopub.status.idle":"2024-04-26T08:39:29.334909Z","shell.execute_reply.started":"2024-04-26T08:39:28.997503Z","shell.execute_reply":"2024-04-26T08:39:29.333449Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Step 1: Changing to Lower Case\ndf1['selftext'] = df1['selftext'].str.lower()\n\n# Step 2: Replacing the Repeating Pattern of '&#039;'\ndf1['selftext'] = df1['selftext'].str.replace(\"&#039;\", \"\")\n\n# Step 3: Removing All Special Characters\ndf1['selftext'] = df1['selftext'].str.replace(r'[^\\w\\d\\s]', '')\n\n# Step 4: Removing Leading and Trailing Whitespaces\ndf1['selftext'] = df1['selftext'].str.strip()\n\n# Step 5: Replacing Multiple Spaces with Single Space\ndf1['selftext'] = df1['selftext'].str.replace(r'\\s+', ' ')\n\n# Display cleaned data\nprint(df1['selftext'])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:29.336649Z","iopub.execute_input":"2024-04-26T08:39:29.337064Z","iopub.status.idle":"2024-04-26T08:39:36.223583Z","shell.execute_reply.started":"2024-04-26T08:39:29.337030Z","shell.execute_reply":"2024-04-26T08:39:36.221754Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"0         does anyone else think the most important part...\n1         hello fellow friends üòÑ\\n\\ni'm on the bpd spect...\n2         my [f20] bf [m20] told me today (after i said ...\n3         okay, i‚Äôm about to open up about many things i...\n4                                                 [removed]\n                                ...                        \n701779    i can't afford a real session and it's 11 pm. ...\n701781    hello. \\n         i'm taking steps to get rid ...\n701782    someone (a war veteran) i know is mentally ill...\n701783                                                  ama\n701786    so i have a lot of random impluses. crazy shit...\nName: selftext, Length: 668054, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"df1.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:36.225504Z","iopub.execute_input":"2024-04-26T08:39:36.225857Z","iopub.status.idle":"2024-04-26T08:39:36.235795Z","shell.execute_reply.started":"2024-04-26T08:39:36.225829Z","shell.execute_reply":"2024-04-26T08:39:36.234060Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Index(['title', 'selftext', 'created_utc', 'over_18', 'subreddit'], dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"# Assuming 'selftext' is one of the columns you expect in df1\n# You should check the actual columns in your DataFrame\n# Make sure to load your DataFrame properly before running these operations\n\n# Check if 'selftext' is in the columns\nif 'selftext' in df1.columns:\n    # Drop rows where 'selftext' is '[removed]' or '\\[removed\\]'\n    df1.drop(df1[(df1['selftext'] =='\\\\[removed\\\\]')].index, inplace=True)\n    df1.drop(df1[(df1['selftext'] =='[removed]')].index, inplace=True)\n\n    # Combine title and text columns\n    df1[\"Sentence\"] = df1[\"title\"] + df1[\"selftext\"]\n\n    # Drop rows with missing values\n    df1.dropna(inplace=True)\n\n    # Drop rows with \"mentalhealth\" subreddit\n    df1.drop(df1[(df1['subreddit'] =='mentalhealth')].index, inplace=True)\n\n    # Select relevant columns\n    df1 = df1[['Sentence', 'subreddit']]\n\n    # Randomly sample 2 rows\n    print(df1.sample(2))\nelse:\n    print(\"'selftext' column not found in DataFrame\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:36.237828Z","iopub.execute_input":"2024-04-26T08:39:36.238222Z","iopub.status.idle":"2024-04-26T08:39:39.241320Z","shell.execute_reply.started":"2024-04-26T08:39:36.238190Z","shell.execute_reply":"2024-04-26T08:39:39.239955Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"                                                 Sentence   subreddit\n308130  I'm in my late 20s learning 18yo mistakes and ...  depression\n514237  Just tested positive for covid and now i know ...     Anxiety\n","output_type":"stream"}]},{"cell_type":"code","source":"# df1['text_total'] = df1['Sentence'].apply(lambda x: len(x.split()))\n\n# def count_total_words(text):\n#     char = 0\n#     for word in text.split():\n#         char += len(word)\n#     return char\n\n# df1['text_chars'] = df1[\"Sentence\"].apply(count_total_words)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:39.243390Z","iopub.execute_input":"2024-04-26T08:39:39.243858Z","iopub.status.idle":"2024-04-26T08:39:39.250154Z","shell.execute_reply.started":"2024-04-26T08:39:39.243809Z","shell.execute_reply":"2024-04-26T08:39:39.248716Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# df1['text_chars']","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:39.252040Z","iopub.execute_input":"2024-04-26T08:39:39.252986Z","iopub.status.idle":"2024-04-26T08:39:39.267138Z","shell.execute_reply.started":"2024-04-26T08:39:39.252934Z","shell.execute_reply":"2024-04-26T08:39:39.265360Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df1.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:39.269063Z","iopub.execute_input":"2024-04-26T08:39:39.269514Z","iopub.status.idle":"2024-04-26T08:39:39.288283Z","shell.execute_reply.started":"2024-04-26T08:39:39.269479Z","shell.execute_reply":"2024-04-26T08:39:39.287053Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                            Sentence subreddit\n0  Life is so pointless without othersdoes anyone...       BPD\n1  Cold rage?hello fellow friends üòÑ\\n\\ni'm on the...       BPD\n2  I don‚Äôt know who I ammy [f20] bf [m20] told me...       BPD\n3  HELP! Opinions! Advice!okay, i‚Äôm about to open...       BPD\n5  My ex got diagnosed with BPDwithout going into...       BPD","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>subreddit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Life is so pointless without othersdoes anyone...</td>\n      <td>BPD</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cold rage?hello fellow friends üòÑ\\n\\ni'm on the...</td>\n      <td>BPD</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I don‚Äôt know who I ammy [f20] bf [m20] told me...</td>\n      <td>BPD</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HELP! Opinions! Advice!okay, i‚Äôm about to open...</td>\n      <td>BPD</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>My ex got diagnosed with BPDwithout going into...</td>\n      <td>BPD</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"# Function to tokenize text using split()\ndef tokenize_text(text):\n    tokens = text.split()\n    return tokens\n\n# Apply the tokenize_text function to your DataFrame column\ndf1['tokenized_review'] = df1['Sentence'].apply(tokenize_text)\n\n# Show the DataFrame with tokenized reviews\nprint(df1['tokenized_review'])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:39:39.289808Z","iopub.execute_input":"2024-04-26T08:39:39.290995Z","iopub.status.idle":"2024-04-26T08:40:03.466238Z","shell.execute_reply.started":"2024-04-26T08:39:39.290950Z","shell.execute_reply":"2024-04-26T08:40:03.464294Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"0         [Life, is, so, pointless, without, othersdoes,...\n1         [Cold, rage?hello, fellow, friends, üòÑ, i'm, on...\n2         [I, don‚Äôt, know, who, I, ammy, [f20], bf, [m20...\n3         [HELP!, Opinions!, Advice!okay,, i‚Äôm, about, t...\n5         [My, ex, got, diagnosed, with, BPDwithout, goi...\n                                ...                        \n701779    [I, really, need, to, talk, to, a, therapist.....\n701781    [I, have, picahello., i'm, taking, steps, to, ...\n701782    [Where, can, you, go, to, get, help, for, some...\n701783                        [I, am, rooster, illusionama]\n701786    [crazy, motherfuckerso, i, have, a, lot, of, r...\nName: tokenized_review, Length: 581215, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport pickle\n\n# Assuming 'data' is your preprocessed DataFrame\n# Save the preprocessed data to a pickle file\nwith open('pdata.pkl', 'wb') as f:\n    pickle.dump(df1, f)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:40:03.468058Z","iopub.execute_input":"2024-04-26T08:40:03.468504Z","iopub.status.idle":"2024-04-26T08:41:01.772311Z","shell.execute_reply.started":"2024-04-26T08:40:03.468470Z","shell.execute_reply":"2024-04-26T08:41:01.770548Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Corpus","metadata":{}},{"cell_type":"code","source":"corpus = df1[['Sentence']].copy()\n\n# Display the corpus\nprint(\"Corpus:\")\nprint(corpus)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:41:01.774835Z","iopub.execute_input":"2024-04-26T08:41:01.775282Z","iopub.status.idle":"2024-04-26T08:41:01.837981Z","shell.execute_reply.started":"2024-04-26T08:41:01.775251Z","shell.execute_reply":"2024-04-26T08:41:01.835789Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Corpus:\n                                                 Sentence\n0       Life is so pointless without othersdoes anyone...\n1       Cold rage?hello fellow friends üòÑ\\n\\ni'm on the...\n2       I don‚Äôt know who I ammy [f20] bf [m20] told me...\n3       HELP! Opinions! Advice!okay, i‚Äôm about to open...\n5       My ex got diagnosed with BPDwithout going into...\n...                                                   ...\n701779  I really need to talk to a therapist..i can't ...\n701781  I have picahello. \\n         i'm taking steps ...\n701782  Where can you go to get help for someone menta...\n701783                           I am rooster illusionama\n701786  crazy motherfuckerso i have a lot of random im...\n\n[581215 rows x 1 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"df1.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:41:01.840270Z","iopub.execute_input":"2024-04-26T08:41:01.840665Z","iopub.status.idle":"2024-04-26T08:41:01.870361Z","shell.execute_reply.started":"2024-04-26T08:41:01.840631Z","shell.execute_reply":"2024-04-26T08:41:01.868404Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"                                            Sentence subreddit  \\\n0  Life is so pointless without othersdoes anyone...       BPD   \n1  Cold rage?hello fellow friends üòÑ\\n\\ni'm on the...       BPD   \n2  I don‚Äôt know who I ammy [f20] bf [m20] told me...       BPD   \n3  HELP! Opinions! Advice!okay, i‚Äôm about to open...       BPD   \n5  My ex got diagnosed with BPDwithout going into...       BPD   \n\n                                    tokenized_review  \n0  [Life, is, so, pointless, without, othersdoes,...  \n1  [Cold, rage?hello, fellow, friends, üòÑ, i'm, on...  \n2  [I, don‚Äôt, know, who, I, ammy, [f20], bf, [m20...  \n3  [HELP!, Opinions!, Advice!okay,, i‚Äôm, about, t...  \n5  [My, ex, got, diagnosed, with, BPDwithout, goi...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>subreddit</th>\n      <th>tokenized_review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Life is so pointless without othersdoes anyone...</td>\n      <td>BPD</td>\n      <td>[Life, is, so, pointless, without, othersdoes,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cold rage?hello fellow friends üòÑ\\n\\ni'm on the...</td>\n      <td>BPD</td>\n      <td>[Cold, rage?hello, fellow, friends, üòÑ, i'm, on...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I don‚Äôt know who I ammy [f20] bf [m20] told me...</td>\n      <td>BPD</td>\n      <td>[I, don‚Äôt, know, who, I, ammy, [f20], bf, [m20...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HELP! Opinions! Advice!okay, i‚Äôm about to open...</td>\n      <td>BPD</td>\n      <td>[HELP!, Opinions!, Advice!okay,, i‚Äôm, about, t...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>My ex got diagnosed with BPDwithout going into...</td>\n      <td>BPD</td>\n      <td>[My, ex, got, diagnosed, with, BPDwithout, goi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport pickle\n\n# Assuming 'data' is your preprocessed DataFrame\n# Save the preprocessed data to a pickle file\nwith open('pdata.pkl', 'wb') as f:\n    pickle.dump(df1, f)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:41:01.872362Z","iopub.execute_input":"2024-04-26T08:41:01.872760Z","iopub.status.idle":"2024-04-26T08:42:11.468621Z","shell.execute_reply.started":"2024-04-26T08:41:01.872719Z","shell.execute_reply":"2024-04-26T08:42:11.467006Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# DTM","metadata":{}},{"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer\n\n# # Create a CountVectorizer object\n# cv = CountVectorizer(stop_words='english')\n\n# # Fit and transform the data to create the document-term matrix\n# data_cv = cv.fit_transform(df1.Sentence)\n\n# # Convert the document-term matrix to a DataFrame\n# data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n\n# # Display the document-term matrix\n# print(data_dtm)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:11.470979Z","iopub.execute_input":"2024-04-26T08:42:11.471656Z","iopub.status.idle":"2024-04-26T08:42:11.476859Z","shell.execute_reply.started":"2024-04-26T08:42:11.471615Z","shell.execute_reply":"2024-04-26T08:42:11.475573Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.feature_extraction.text import CountVectorizer\n\n# # Define or load your DataFrame 'df1' with text data\n# data = df1['Sentence']\n\n# # Create the DataFrame\n# df2 = pd.DataFrame(data)\n\n# # Create a CountVectorizer object\n# cv = CountVectorizer(stop_words='english')\n\n# # Fit and transform the data to create the document-term matrix\n# data_cv = cv.fit_transform(df2.Sentence)\n\n# # Convert the document-term matrix to a DataFrame\n# data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n\n# # Display the document-term matrix\n# print(data_dtm)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:11.479870Z","iopub.execute_input":"2024-04-26T08:42:11.480785Z","iopub.status.idle":"2024-04-26T08:42:11.494332Z","shell.execute_reply.started":"2024-04-26T08:42:11.480745Z","shell.execute_reply":"2024-04-26T08:42:11.493235Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# import pickle\n# pickle.dump(cv, open(\"cv.pkl\", \"wb\"))","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:11.495836Z","iopub.execute_input":"2024-04-26T08:42:11.496635Z","iopub.status.idle":"2024-04-26T08:42:11.508868Z","shell.execute_reply.started":"2024-04-26T08:42:11.496599Z","shell.execute_reply":"2024-04-26T08:42:11.507080Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# # Create a CountVectorizer object with specified parameters\n# cv = CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=0.1, max_df=0.8)\n\n# # Fit and transform the data to create the document-term matrix\n# data_cv = cv.fit_transform(df1.Sentence)\n\n# # Convert the document-term matrix to a DataFrame\n# data_dtm2 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n\n# # Display the document-term matrix\n# print(data_dtm2","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:11.510942Z","iopub.execute_input":"2024-04-26T08:42:11.511752Z","iopub.status.idle":"2024-04-26T08:42:11.524630Z","shell.execute_reply.started":"2024-04-26T08:42:11.511698Z","shell.execute_reply":"2024-04-26T08:42:11.522843Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# print(data_dtm.shape) \n# print(data_dtm2.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:11.533738Z","iopub.execute_input":"2024-04-26T08:42:11.534249Z","iopub.status.idle":"2024-04-26T08:42:11.540427Z","shell.execute_reply.started":"2024-04-26T08:42:11.534215Z","shell.execute_reply":"2024-04-26T08:42:11.538900Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer\n# import pandas as pd\n\n# # Sample data\n# data = {\n#     'Sentence': [\n#         \"Life is so pointless without othersdoes anyone...\",\n#         \"Cold rage?hello fellow friends üòÑ\\n\\ni'm on the...\",\n#         \"I don‚Äôt know who I ammy [f20] bf [m20] told me...\",\n#         \"HELP! Opinions! Advice!okay, i‚Äôm about to open...\",\n#         \"My ex got diagnosed with BPDwithout going into...\"],\n#     'subreddit': ['BPD', 'BPD', 'BPD', 'BPD', 'BPD']\n# }\n\n# # Create DataFrame\n# df = pd.DataFrame(data)\n\n# # Step 1: Create a CountVectorizer object\n# cv = CountVectorizer(stop_words='english')\n\n# # Step 2: Fit and transform the data to create the document-term matrix\n# data_cv = cv.fit_transform(df['Sentence'])\n\n# # Step 3: Convert the document-term matrix to a DataFrame\n# data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n\n# # Display the document-term matrix\n# print(data_dtm)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:11.542498Z","iopub.execute_input":"2024-04-26T08:42:11.543331Z","iopub.status.idle":"2024-04-26T08:42:11.553945Z","shell.execute_reply.started":"2024-04-26T08:42:11.543284Z","shell.execute_reply":"2024-04-26T08:42:11.551882Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\n# Assuming df1 is your DataFrame with more than 100 entries\n# Sample data (assuming df1 is your DataFrame)\ndf1_sample = df1.head(100)  # Selecting the first 100 entries\n\n# Step 1: Create a CountVectorizer object\ncv = CountVectorizer(stop_words='english')\n\n# Step 2: Fit and transform the data to create the document-term matrix\ndata_cv = cv.fit_transform(df1_sample['Sentence'])\n\n# Step 3: Convert the document-term matrix to a DataFrame\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n\n# Display the document-term matrix\nprint(data_dtm)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:11.558164Z","iopub.execute_input":"2024-04-26T08:42:11.558902Z","iopub.status.idle":"2024-04-26T08:42:12.212897Z","shell.execute_reply.started":"2024-04-26T08:42:11.558854Z","shell.execute_reply":"2024-04-26T08:42:12.211292Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"    10  100  1000  12  13  14  15  150mg  16  17  ...  yes  yesterday  yo  \\\n0    0    0     0   0   0   0   0      0   0   0  ...    0          0   0   \n1    0    0     0   0   0   1   0      0   0   0  ...    0          0   1   \n2    0    0     0   0   0   0   0      0   0   0  ...    0          0   0   \n3    0    1     0   0   0   0   0      0   0   2  ...    0          0   0   \n4    0    0     0   0   0   0   0      0   0   0  ...    0          0   0   \n..  ..  ...   ...  ..  ..  ..  ..    ...  ..  ..  ...  ...        ...  ..   \n95   0    0     0   0   0   0   0      0   0   0  ...    0          0   0   \n96   0    0     0   0   0   0   0      0   0   0  ...    0          0   0   \n97   0    0     0   0   0   0   0      0   0   0  ...    0          0   0   \n98   0    0     0   0   0   0   0      0   0   0  ...    0          0   0   \n99   0    0     0   0   0   0   0      0   0   0  ...    0          0   0   \n\n    younger  youtube  yrs  zingers  zoloft  zombified  zoom  \n0         0        0    0        0       0          0     0  \n1         0        0    0        0       0          0     0  \n2         0        0    0        0       0          0     0  \n3         0        0    0        0       0          0     0  \n4         0        0    0        0       0          0     0  \n..      ...      ...  ...      ...     ...        ...   ...  \n95        0        0    0        0       0          0     0  \n96        0        0    0        0       0          0     0  \n97        0        0    0        0       0          0     0  \n98        0        0    0        0       0          0     0  \n99        0        0    0        0       0          0     0  \n\n[100 rows x 2521 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\npickle.dump(cv, open(\"dtm.pkl\", \"wb\"))","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:30.885039Z","iopub.execute_input":"2024-04-26T08:42:30.886175Z","iopub.status.idle":"2024-04-26T08:42:30.896472Z","shell.execute_reply.started":"2024-04-26T08:42:30.886100Z","shell.execute_reply":"2024-04-26T08:42:30.894225Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# Create a CountVectorizer object with specified parameters\ncv = CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=0.1, max_df=0.8)\n\n# Fit and transform the data to create the document-term matrix\ndata_cv = cv.fit_transform(df1.Sentence)\n\n# Convert the document-term matrix to a DataFrame\ndata_dtm2 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n\n# Display the document-term matrix\nprint(data_dtm2)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:57.524697Z","iopub.execute_input":"2024-04-26T08:42:57.525457Z","iopub.status.idle":"2024-04-26T08:46:35.954079Z","shell.execute_reply.started":"2024-04-26T08:42:57.525419Z","shell.execute_reply":"2024-04-26T08:46:35.952703Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"        ago  anxiety  anymore  away  bad  better  bpd  day  days  depression  \\\n0         0        0        0     0    0       0    0    0     0           0   \n1         0        0        1     0    0       0    1    1     0           0   \n2         0        0        1     0    0       2    1    0     0           0   \n3         1        1        1     2    3       0    0    3     1           0   \n4         0        0        0     1    0       0    0    0     0           0   \n...     ...      ...      ...   ...  ...     ...  ...  ...   ...         ...   \n581210    0        0        0     0    0       0    0    0     0           0   \n581211    0        0        0     0    0       0    0    0     0           0   \n581212    0        0        0     0    0       0    0    0     0           0   \n581213    0        0        0     0    0       0    0    0     0           0   \n581214    0        0        0     0    0       0    0    0     0           0   \n\n        ...  times  told  try  trying  ve  want  way  work  year  years  \n0       ...      0     0    0       0   0     0    0     0     0      0  \n1       ...      0     1    1       1   0     0    2     1     0      0  \n2       ...      0     1    0       0   0     0    0     0     0      0  \n3       ...      1     2    1       0   1     2    0     0     0      1  \n4       ...      0     1    0       0   2     0    0     1     0      0  \n...     ...    ...   ...  ...     ...  ..   ...  ...   ...   ...    ...  \n581210  ...      0     0    0       0   0     0    0     0     0      0  \n581211  ...      0     1    0       0   1     0    0     0     0      0  \n581212  ...      0     0    0       0   0     1    0     0     0      0  \n581213  ...      0     0    0       0   0     0    0     0     0      0  \n581214  ...      0     0    0       0   0     3    0     0     0      0  \n\n[581215 rows x 79 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the DataFrame to a pickle file\ndata_dtm2.to_pickle('/kaggle/working/dtm2.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:49:26.880916Z","iopub.execute_input":"2024-04-26T08:49:26.881747Z","iopub.status.idle":"2024-04-26T08:49:27.270293Z","shell.execute_reply.started":"2024-04-26T08:49:26.881710Z","shell.execute_reply":"2024-04-26T08:49:27.268998Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Assuming data_dtm2 is your DataFrame representing the document-term matrix\ndata_dtm2.to_csv('doc.txt', sep='\\t', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:49:29.121062Z","iopub.execute_input":"2024-04-26T08:49:29.121587Z","iopub.status.idle":"2024-04-26T08:49:45.081204Z","shell.execute_reply.started":"2024-04-26T08:49:29.121551Z","shell.execute_reply":"2024-04-26T08:49:45.079687Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"print(data_dtm.shape) \nprint(data_dtm2.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:49:49.761353Z","iopub.execute_input":"2024-04-26T08:49:49.762503Z","iopub.status.idle":"2024-04-26T08:49:49.770011Z","shell.execute_reply.started":"2024-04-26T08:49:49.762450Z","shell.execute_reply":"2024-04-26T08:49:49.768223Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"(100, 2521)\n(581215, 79)\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Create a CountVectorizer object with specified parameters\n# cv = CountVectorizer(stop_words='english', ngram_range=(1, 2), min_df=0.1, max_df=0.8)\n\n# # Fit and transform the data to create the document-term matrix\n# data_cv = cv.fit_transform(corpus['Sentence'])\n\n# # Convert the document-term matrix to a DataFrame\n# data_dtm2 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n\n# # Display the document-term matrix\n# print(data_dtm2)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:12.226068Z","iopub.execute_input":"2024-04-26T08:42:12.226496Z","iopub.status.idle":"2024-04-26T08:42:12.246152Z","shell.execute_reply.started":"2024-04-26T08:42:12.226465Z","shell.execute_reply":"2024-04-26T08:42:12.245030Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# print(data_dtm.shape) \n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:12.248715Z","iopub.execute_input":"2024-04-26T08:42:12.249310Z","iopub.status.idle":"2024-04-26T08:42:12.263698Z","shell.execute_reply.started":"2024-04-26T08:42:12.249252Z","shell.execute_reply":"2024-04-26T08:42:12.262612Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# # Apply a second round of cleaning\n# def clean_text_round2(text):\n#     '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n#     text = re.sub('[‚Äò‚Äô‚Äú‚Äù‚Ä¶]', '', text)\n#     text = re.sub('\\n', '', text)\n#     return text\n\n# round2 = lambda x: clean_text_round2(df['review'])","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:12.265696Z","iopub.execute_input":"2024-04-26T08:42:12.266554Z","iopub.status.idle":"2024-04-26T08:42:12.276145Z","shell.execute_reply.started":"2024-04-26T08:42:12.266508Z","shell.execute_reply":"2024-04-26T08:42:12.274878Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# df = data_dtm.transpose()\n# df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:12.278721Z","iopub.execute_input":"2024-04-26T08:42:12.279721Z","iopub.status.idle":"2024-04-26T08:42:12.289734Z","shell.execute_reply.started":"2024-04-26T08:42:12.279675Z","shell.execute_reply":"2024-04-26T08:42:12.288167Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n\n# import matplotlib.pyplot as plt\n# import seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:12.291440Z","iopub.execute_input":"2024-04-26T08:42:12.291947Z","iopub.status.idle":"2024-04-26T08:42:12.302787Z","shell.execute_reply.started":"2024-04-26T08:42:12.291898Z","shell.execute_reply":"2024-04-26T08:42:12.301623Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# from wordcloud import WordCloud\n# from wordcloud import STOPWORDS\n\n# stopwords = set(STOPWORDS)\n\n# wordcloud = WordCloud(background_color = 'orange', stopwords = stopwords, width = 1200, height = 800).generate(str(df1['tokenized_review']))\n\n# plt.rcParams['figure.figsize'] = (15, 15)\n# plt.title('Word Cloud - Drug Names', fontsize = 25)\n# print(wordcloud)\n# plt.axis('off')\n# plt.imshow(wordcloud)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:12.305022Z","iopub.execute_input":"2024-04-26T08:42:12.306380Z","iopub.status.idle":"2024-04-26T08:42:12.319441Z","shell.execute_reply.started":"2024-04-26T08:42:12.306331Z","shell.execute_reply":"2024-04-26T08:42:12.318029Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# from collections import Counter\n# import nltk\n# from nltk.corpus import stopwords\n\n# nltk.download('punkt')\n# nltk.download('stopwords')\n\n# # Load your dataset into a DataFrame\n\n\n# # Tokenize and process the reviews\n# stop_words = set(stopwords.words('english'))\n\n# # Function to remove stopwords and filter out non-alphanumeric words\n# def process_review(review):\n#     return [word for word in review if word.isalnum() and word not in stop_words]\n\n# # Apply processing function to each review\n# processed_reviews = df1['tokenized_review'].apply(process_review)\n\n# # Flatten the list of processed reviews\n# all_words = [word for sublist in processed_reviews for word in sublist]\n\n# # Count the frequency of words\n# word_counts = Counter(all_words)\n\n# # Get the top 30 words\n# top_words = word_counts.most_common(30)\n\n# # Display the top words\n# print(top_words)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:12.321590Z","iopub.execute_input":"2024-04-26T08:42:12.322372Z","iopub.status.idle":"2024-04-26T08:42:12.333101Z","shell.execute_reply.started":"2024-04-26T08:42:12.322325Z","shell.execute_reply":"2024-04-26T08:42:12.331409Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# all_words = [word for sublist in df1['tokenized_review'] for word in sublist]\n\n# # Remove stopwords\n# stop_words = set(stopwords.words('english'))\n# filtered_words = [word for word in all_words if word not in stop_words]\n\n# # Count the frequency of words\n# word_counts = Counter(filtered_words)\n\n# # Get the top 30 words\n# top_words = word_counts.most_common(30)\n\n# # Display the top words\n# print(top_words)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:12.335251Z","iopub.execute_input":"2024-04-26T08:42:12.335747Z","iopub.status.idle":"2024-04-26T08:42:12.353020Z","shell.execute_reply.started":"2024-04-26T08:42:12.335712Z","shell.execute_reply":"2024-04-26T08:42:12.351598Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# from wordcloud import WordCloud\n# from wordcloud import STOPWORDS\n\n# stopwords = set(STOPWORDS)\n\n# wordcloud = WordCloud(background_color = 'orange', stopwords = stopwords, width = 1200, height = 800).generate(str(df1['tokenized_review']))\n\n# plt.rcParams['figure.figsize'] = (15, 15)\n# plt.title('Word Cloud - Sentences', fontsize = 25)\n# print(wordcloud)\n# plt.axis('off')\n# plt.imshow(wordcloud)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:12.354972Z","iopub.execute_input":"2024-04-26T08:42:12.355412Z","iopub.status.idle":"2024-04-26T08:42:12.366888Z","shell.execute_reply.started":"2024-04-26T08:42:12.355370Z","shell.execute_reply":"2024-04-26T08:42:12.365038Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# from collections import Counter\n\n# # Assuming 'reviews' is a list of review texts\n# reviews = df1['tokenized_review']\n\n# # Define a Counter to count the occurrences of each word\n# word_counter = Counter()\n\n# # Iterate through each review and update the word counter\n# for review in reviews:\n#     words = review.split()  # Split the review into words\n#     word_counter.update(words)  # Update the counter with the words in the review\n\n# # Print the top 15 words\n# top_words = word_counter.most_common(15)\n# for word, count in top_words:\n#     print(f\"{word}: {count}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:12.368249Z","iopub.execute_input":"2024-04-26T08:42:12.368687Z","iopub.status.idle":"2024-04-26T08:42:12.378859Z","shell.execute_reply.started":"2024-04-26T08:42:12.368653Z","shell.execute_reply":"2024-04-26T08:42:12.377439Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# from collections import Counter\n\n# # Sample reviews list\n# reviews = [\"This is a sample review\", \"Another sample review\", \"Yet another review\"]\n\n# # Initialize a word counter\n# word_counter = Counter()\n\n# # Iterate through each review and update the word counter\n# for review in reviews:\n#     if isinstance(review, str):  # Check if the element is a string\n#         words = review.split()  # Split the review into words\n#         word_counter.update(words)  # Update the counter with the words in the review\n\n# # Print the top 15 words\n# print(word_counter.most_common(15))","metadata":{"execution":{"iopub.status.busy":"2024-04-26T08:42:12.380996Z","iopub.execute_input":"2024-04-26T08:42:12.381616Z","iopub.status.idle":"2024-04-26T08:42:12.397353Z","shell.execute_reply.started":"2024-04-26T08:42:12.381568Z","shell.execute_reply":"2024-04-26T08:42:12.396224Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}